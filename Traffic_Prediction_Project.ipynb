{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SriNihitha12/BigdataProject/blob/main/Traffic_Prediction_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **Linear Regression**\n",
        "\n",
        "### What it is:\n",
        "A simple, classic machine learning model that predicts a continuous output (like traffic volume) based on a straight-line relationship between input features and output.\n",
        "\n",
        "### How it works:\n",
        "It finds the best-fitting line through the data points by minimizing the error between predicted and actual values using Least Squares.\n",
        "\n",
        "### Why used here:\n",
        "To quickly build a simple traffic volume predictor using basic features (hour, weekday, month).\n"
      ],
      "metadata": {
        "id": "Mhi-OTJwcamL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "avk4FhIRY0Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Install necessary Python libraries\n",
        "!pip install pandas matplotlib scikit-learn"
      ],
      "metadata": {
        "id": "ilfJQQBMY1NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Import required libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "gZ2rZ6HFasFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Define the path to the dataset file in Google Drive\n",
        "data_path = '/content/drive/MyDrive/TrafficData/Minnesota_TrafficData_2020-24.csv'"
      ],
      "metadata": {
        "id": "Q2MQ3kVHasKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Load the traffic dataset using pandas\n",
        "df = pd.read_csv(data_path)"
      ],
      "metadata": {
        "id": "Sf3DDv2YasPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Display first few rows (for checking structure)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cp3GeaKFasUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Remove any fully empty rows\n",
        "df.dropna(how='all', inplace=True)"
      ],
      "metadata": {
        "id": "CChkyjCXasX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Optional - Print column names to verify hour columns exist\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "7tjde_5Basbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Reshape wide format (1â€“24 hours) into long format using melt\n",
        "hourly_df = df.melt(\n",
        "    id_vars=['station_id', 'dir_of_travel', 'lane_of_travel', 'date'],\n",
        "    value_vars=[str(i) for i in range(1, 25)],\n",
        "    var_name='hour',\n",
        "    value_name='volume'\n",
        ")"
      ],
      "metadata": {
        "id": "9V-2cz67aseq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: Combine date and hour to create a full datetime column\n",
        "hourly_df['datetime'] = pd.to_datetime(hourly_df['date']) + pd.to_timedelta(hourly_df['hour'].astype(int) - 1, unit='h')"
      ],
      "metadata": {
        "id": "OByH4-0QasiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 11: Sort data by datetime and reset the index\n",
        "hourly_df.sort_values('datetime', inplace=True)\n",
        "hourly_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "WnbSzh9easlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 12: Drop rows where volume is missing\n",
        "hourly_df = hourly_df[hourly_df['volume'].notnull()]"
      ],
      "metadata": {
        "id": "z3oHbPJzaspO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 13: Show structure of cleaned data\n",
        "hourly_df.head()"
      ],
      "metadata": {
        "id": "-JR0_-Drassi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 14: Plot a sample of traffic volume over time\n",
        "plt.figure(figsize=(14, 4))\n",
        "plt.plot(hourly_df['datetime'][:500], hourly_df['volume'][:500])\n",
        "plt.title(\"Traffic Volume Over Time (Sample)\")\n",
        "plt.xlabel(\"Datetime\")\n",
        "plt.ylabel(\"Volume\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a1Flurptap0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 15: Extract time-related features: hour, weekday, and month\n",
        "hourly_df['hour'] = hourly_df['datetime'].dt.hour\n",
        "hourly_df['weekday'] = hourly_df['datetime'].dt.weekday\n",
        "hourly_df['month'] = hourly_df['datetime'].dt.month"
      ],
      "metadata": {
        "id": "_STgKXXPap_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 16: Prepare input features and target variable\n",
        "features = hourly_df[['hour', 'weekday', 'month']]\n",
        "target = hourly_df['volume']"
      ],
      "metadata": {
        "id": "UdRJxPxlaqHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 17: Split data into training and test sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "1uz2aty_aqPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 18: Initialize and train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "gr6Kn3_0aqU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 19: Use the trained model to predict on the test data\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "8ofPRDKlaqaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 20: Calculate and print evaluation metrics: MSE and RMSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"LinearRegression RMSE:\", rmse)"
      ],
      "metadata": {
        "id": "ZuM6GMTEaqfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 21: Plot a comparison of actual vs predicted traffic volumes (sample 100)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(y_test.values[:100], label='Actual')\n",
        "plt.plot(y_pred[:100], label='Predicted')\n",
        "plt.title(\"Actual vs Predicted Traffic Volume\")\n",
        "plt.xlabel(\"Sample\")\n",
        "plt.ylabel(\"Volume\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BmdNVfNJaqiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "### What it is:\n",
        "A powerful boosting algorithm that builds a series of decision trees, each correcting the previous one's errors.\n",
        "\n",
        "### How it works:\n",
        "It sequentially improves trees using gradient descent to minimize loss.\n",
        "\n",
        "### Why used here:\n",
        "To achieve high accuracy on large, complex traffic datasets efficiently."
      ],
      "metadata": {
        "id": "J-R3XywdcoAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBoost\n",
        "# Mount Google Drive to access files (If you run this before no need to run here, if you want to run this model you should run this first)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NWp90fd7aqod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP1. Install XGBoost\n",
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "t8h7gwGGaqr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP2. Import necessary libraries\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "kos0jlE0aqwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP3. Create and train the XGBoost model\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "bEgLUX-_aq0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP4. Make predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "81fNdUwGaq3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP5. Evaluate the model using RMSE\n",
        "xgb_mse = mean_squared_error(y_test, y_pred_xgb)\n",
        "xgb_rmse = np.sqrt(xgb_mse)\n",
        "# Print RMSE\n",
        "print(\" XGBoost RMSE:\", xgb_rmse)"
      ],
      "metadata": {
        "id": "_E_gVLQsY1QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Create a pivot table for heatmap visualization (Predicted volume)\n",
        "predicted_pivot = pd.DataFrame(y_pred, columns=['Predicted'], index=X_test.index)\n",
        "predicted_pivot['hour'] = X_test['hour']\n",
        "predicted_pivot['weekday'] = X_test['weekday']\n",
        "predicted_pivot = predicted_pivot.pivot_table(values='Predicted', index='hour', columns='weekday', aggfunc='mean')"
      ],
      "metadata": {
        "id": "ZfVkNCx9Y1Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 7: Plotting the heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(predicted_pivot, cmap='coolwarm', annot=True, fmt='.0f', cbar_kws={'label': 'Predicted Volume'})\n",
        "plt.title(\"Predicted Traffic Volume by Hour & Weekday\")\n",
        "plt.xlabel(\"Weekday (0 = Monday)\")\n",
        "plt.ylabel(\"Hour\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "arVDLVkkY1Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Decision Tree Regressor\n",
        "\n",
        "### What it is:\n",
        "A simple tree structure that splits data based on feature values to make predictions.\n",
        "\n",
        "### How it works:\n",
        "It asks a series of questions at nodes and assigns a value at leaves based on data splits.\n",
        "\n",
        "### Why used here:\n",
        "To easily visualize and understand how time features affect traffic volume."
      ],
      "metadata": {
        "id": "l8hRHBPzgHc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Mount Google Drive to access your dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_APZPDvOY237"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Install required libraries\n",
        "!pip install pandas matplotlib seaborn scikit-learn"
      ],
      "metadata": {
        "id": "2O87OK-IY3AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "eDwkSX0dY3I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Load Dataset\n",
        "data_path = '/content/drive/MyDrive/TrafficData/Minnesota_TrafficData_2020-24.csv'  # Update if needed\n",
        "df = pd.read_csv(data_path)\n",
        "df.dropna(how='all', inplace=True)"
      ],
      "metadata": {
        "id": "DGP4qH6wY3Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Convert wide format (1-24 hours) to long format\n",
        "hourly_df = df.melt(\n",
        "    id_vars=['station_id', 'dir_of_travel', 'lane_of_travel', 'date'],\n",
        "    value_vars=[str(i) for i in range(1, 25)],\n",
        "    var_name='hour',\n",
        "    value_name='volume'\n",
        ")"
      ],
      "metadata": {
        "id": "hSla3t-_Y1Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Generate datetime and extract features\n",
        "hourly_df['datetime'] = pd.to_datetime(hourly_df['date']) + pd.to_timedelta(hourly_df['hour'].astype(int) - 1, unit='h')\n",
        "hourly_df.dropna(subset=['volume'], inplace=True)\n",
        "hourly_df['hour'] = hourly_df['datetime'].dt.hour\n",
        "hourly_df['weekday'] = hourly_df['datetime'].dt.weekday\n",
        "hourly_df['month'] = hourly_df['datetime'].dt.month"
      ],
      "metadata": {
        "id": "n_f8in2EY1bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Prepare input features and target\n",
        "features = hourly_df[['hour', 'weekday', 'month']]\n",
        "target = hourly_df['volume']\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "lLZQEHH2Y1d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Train Decision Tree Regressor\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "dt_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "xaKc6k5jY1g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Predict and Evaluate\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
        "print(\"RMSE (Decision Tree Regressor):\", rmse_dt)"
      ],
      "metadata": {
        "id": "3nKb8KrgY1jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: Create pivot table for heatmap\n",
        "dt_heatmap_df = X_test.copy()\n",
        "dt_heatmap_df['Predicted'] = y_pred_dt\n",
        "dt_pivot = dt_heatmap_df.pivot_table(\n",
        "    values='Predicted',\n",
        "    index='hour',\n",
        "    columns='weekday',\n",
        "    aggfunc='mean'\n",
        ")"
      ],
      "metadata": {
        "id": "nMmlGkC7Y1mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 11: Plot the heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(dt_pivot, cmap='YlOrRd', annot=True, fmt='.0f', cbar_kws={'label': 'Predicted Volume'})\n",
        "plt.title(\"Decision Tree: Avg Predicted Traffic Volume by Hour & Weekday\")\n",
        "plt.xlabel(\"Weekday (0 = Monday)\")\n",
        "plt.ylabel(\"Hour\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wn2eV3sGY1ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Prophet\n",
        "\n",
        "### What it is:\n",
        "A time-series forecasting tool developed by Facebook, best for capturing trends and seasonality.\n",
        "\n",
        "### How it works:\n",
        "It automatically models trend, yearly, weekly seasonality, and holiday effects.\n",
        "\n",
        "### Why used here:\n",
        "To predict future traffic patterns based on strong daily and weekly cycles."
      ],
      "metadata": {
        "id": "bLIQz4ffhcpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access your dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Iq8UvX_4Y1rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install Prophet\n",
        "!pip install prophet --quiet"
      ],
      "metadata": {
        "id": "loE1Oc3qao0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import ListedColormap"
      ],
      "metadata": {
        "id": "Pj66RALaY1uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Load the Dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/TrafficData/Minnesota_TrafficData_2020-24.csv')\n",
        "df.dropna(how='all', inplace=True)"
      ],
      "metadata": {
        "id": "XVSoPMoyY1w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Convert Wide to Long\n",
        "hourly_df = df.melt(\n",
        "    id_vars=['station_id', 'dir_of_travel', 'lane_of_travel', 'date'],\n",
        "    value_vars=[str(i) for i in range(1, 25)],\n",
        "    var_name='hour',\n",
        "    value_name='volume'\n",
        ")"
      ],
      "metadata": {
        "id": "kPZc-sM2Y1zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Create Datetime Column\n",
        "hourly_df['datetime'] = pd.to_datetime(hourly_df['date']) + pd.to_timedelta(hourly_df['hour'].astype(int) - 1, unit='h')\n",
        "hourly_df.dropna(subset=['volume'], inplace=True)\n",
        "\n",
        "# Extract hour and weekday for later heatmap classification\n",
        "hourly_df['hour'] = hourly_df['datetime'].dt.hour\n",
        "hourly_df['weekday'] = hourly_df['datetime'].dt.weekday"
      ],
      "metadata": {
        "id": "VmygVcy-Y12j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Prepare Data for Prophet\n",
        "prophet_df = hourly_df[['datetime', 'volume']].rename(columns={'datetime': 'ds', 'volume': 'y'})\n",
        "prophet_df = prophet_df.groupby('ds').mean().reset_index()"
      ],
      "metadata": {
        "id": "6S_OBA-vY15K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Fit the Prophet Model\n",
        "model = Prophet()\n",
        "model.fit(prophet_df)"
      ],
      "metadata": {
        "id": "RMM1FGjpY17u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Forecast Future (48 hours)\n",
        "future = model.make_future_dataframe(periods=48, freq='H')\n",
        "forecast = model.predict(future)"
      ],
      "metadata": {
        "id": "jyhVNOrPaoDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Plot Forecast\n",
        "model.plot(forecast)\n",
        "plt.title(\" Prophet Traffic Volume Forecast\")\n",
        "plt.xlabel(\"Datetime\")\n",
        "plt.ylabel(\"Volume\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mNOyyCXDioKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Forecast Components\n",
        "model.plot_components(forecast)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BJTN4yPiioG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: Calculate RMSE\n",
        "actual = prophet_df['y'].values\n",
        "predicted = model.predict(prophet_df)['yhat'].values\n",
        "rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
        "print(\" Prophet RMSE:\", rmse)"
      ],
      "metadata": {
        "id": "ZMjf89KcioEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 11: Merge back predictions to add hour and weekday\n",
        "forecast_trimmed = forecast[['ds', 'yhat']].copy()\n",
        "forecast_trimmed['hour'] = forecast_trimmed['ds'].dt.hour\n",
        "forecast_trimmed['weekday'] = forecast_trimmed['ds'].dt.weekday"
      ],
      "metadata": {
        "id": "6Br6tTG4ioBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 12: Classify Volume Levels\n",
        "def classify_traffic(v):\n",
        "    if v < 250:\n",
        "        return 'Low'\n",
        "    elif v < 500:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'High'\n",
        "\n",
        "forecast_trimmed['Traffic_Level'] = forecast_trimmed['yhat'].apply(classify_traffic)"
      ],
      "metadata": {
        "id": "iwCA5iZCin-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 13: Pivot Table for Heatmap\n",
        "pivot = forecast_trimmed.pivot_table(\n",
        "    values='Traffic_Level',\n",
        "    index='hour',\n",
        "    columns='weekday',\n",
        "    aggfunc=lambda x: x.mode()[0] if not x.mode().empty else 'Low'\n",
        ")"
      ],
      "metadata": {
        "id": "YNalPL6oin8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 14: Convert Categories to Numeric for Plotting\n",
        "traffic_map = {'Low': 0, 'Medium': 1, 'High': 2}\n",
        "pivot_numeric = pivot.replace(traffic_map)\n",
        "\n",
        "# Color map for 'Low', 'Medium', 'High'\n",
        "color_map = {'Low': '#9be7ff', 'Medium': '#ffc857', 'High': '#ff5c5c'}\n",
        "custom_cmap = ListedColormap([color_map['Low'], color_map['Medium'], color_map['High']])"
      ],
      "metadata": {
        "id": "S953LUlsin5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 15: Plot Classification Heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(\n",
        "    pivot_numeric,\n",
        "    cmap=custom_cmap,\n",
        "    annot=pivot,\n",
        "    fmt='',\n",
        "    cbar=False\n",
        ")\n",
        "plt.title(\" Prophet - Traffic Intensity by Hour & Weekday\")\n",
        "plt.xlabel(\"Weekday (0 = Monday)\")\n",
        "plt.ylabel(\"Hour\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YhfLKw-8in2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Random Forest\n",
        "\n",
        "### What it is:\n",
        "An ensemble method that builds multiple decision trees and averages their results to make a final prediction.\n",
        "\n",
        "### How it works:\n",
        "Each tree is trained on a random sample of the data. By combining the outputs of many trees, it reduces overfitting and improves accuracy.\n",
        "\n",
        "### Why used here:\n",
        "To model complex relationships between time features (hour, weekday, month) and traffic volume with strong generalization."
      ],
      "metadata": {
        "id": "m49LFtc3jUF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Mount Google Drive to access your dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mpgTE7mTin0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Install libraries\n",
        "!pip install pandas openpyxl"
      ],
      "metadata": {
        "id": "0Iz1Kssvinxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Import Random Forest Regressor from sklearn\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "uSpEK-RWinvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create a Random Forest model with 100 trees\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)"
      ],
      "metadata": {
        "id": "_bCu5JSiinsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train the model using training data\n",
        "rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "I1q8W-UJinqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Predict traffic volume using the test data\n",
        "y_pred_rf = rf.predict(X_test)"
      ],
      "metadata": {
        "id": "Y_DPdfKfinng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluate and print RMSE for Random Forest model\n",
        "print(\"RMSE (Random Forest):\", np.sqrt(mean_squared_error(y_test, y_pred_rf)))"
      ],
      "metadata": {
        "id": "ZMuAC-TJinka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Create a pivot table to calculate average actual traffic volume by hour and weekday\n",
        "pivot = hourly_df.pivot_table(values='volume', index='hour', columns='weekday', aggfunc='mean')"
      ],
      "metadata": {
        "id": "SIEkmb4hinhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Set up the heatmap figure\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Average Traffic Volume by Hour & Weekday\")"
      ],
      "metadata": {
        "id": "hAVVN2hdinex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Import seaborn and create the heatmap\n",
        "import seaborn as sns\n",
        "sns.heatmap(pivot, cmap='YlGnBu', annot=True, fmt='.0f')"
      ],
      "metadata": {
        "id": "SSZ7N2hsincC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Label the axes\n",
        "plt.xlabel(\"Weekday (0 = Monday)\")\n",
        "plt.ylabel(\"Hour\")"
      ],
      "metadata": {
        "id": "zJJO-WlyinZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oZzP1M_LinWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. CatBoost\n",
        "\n",
        "### What it is:\n",
        "A machine learning model similar to XGBoost, but especially good at handling categorical data (like weekday, hour) automatically.\n",
        "\n",
        "### How it works:\n",
        "It builds boosting trees like XGBoost but internally treats categorical variables without needing manual conversion (encoding).\n",
        "\n",
        "### Why used here:\n",
        "To accurately predict traffic volume using time-based categorical features like weekday without heavy preprocessing."
      ],
      "metadata": {
        "id": "iYP6wXLrlH8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access your dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b-TJxf8EinTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install CatBoost\n",
        "!pip install catboost"
      ],
      "metadata": {
        "id": "4BucBO6GinQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from matplotlib.colors import ListedColormap"
      ],
      "metadata": {
        "id": "oCQH4BqvinKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Load Dataset\n",
        "data_path = '/content/drive/MyDrive/TrafficData/Minnesota_TrafficData_2020-24.csv'  # Update your path if needed\n",
        "df = pd.read_csv(data_path)\n",
        "df.dropna(how='all', inplace=True)"
      ],
      "metadata": {
        "id": "RXsUUlavim_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Convert wide format to long format\n",
        "id_vars = ['station_id', 'dir_of_travel', 'lane_of_travel', 'date']\n",
        "value_vars = [str(i) for i in range(1, 25)]  # columns '1' to '24'\n",
        "\n",
        "hourly_df = pd.melt(df, id_vars=id_vars, value_vars=value_vars,\n",
        "                    var_name='hour', value_name='volume')"
      ],
      "metadata": {
        "id": "bqXATwiSilzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Extract datetime features\n",
        "hourly_df['datetime'] = pd.to_datetime(hourly_df['date']) + pd.to_timedelta(hourly_df['hour'].astype(int) - 1, unit='h')\n",
        "hourly_df['hour'] = hourly_df['datetime'].dt.hour\n",
        "hourly_df['weekday'] = hourly_df['datetime'].dt.weekday\n",
        "hourly_df['month'] = hourly_df['datetime'].dt.month\n",
        "hourly_df.dropna(subset=['volume'], inplace=True)"
      ],
      "metadata": {
        "id": "PUN4W2GdijWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Prepare Features\n",
        "features = hourly_df[['hour', 'weekday', 'month']]\n",
        "target = hourly_df['volume']\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "VPNQBJ5cmK7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Train CatBoost Regressor\n",
        "cat_model = CatBoostRegressor(verbose=0)\n",
        "cat_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "l4QfjKcymKuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Predict and Evaluate\n",
        "y_pred = cat_model.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\" CatBoost RMSE:\", rmse)"
      ],
      "metadata": {
        "id": "Rmo2LVzPmKg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Prediction Plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(y_test.values[:100], label='Actual')\n",
        "plt.plot(y_pred[:100], label='Predicted')\n",
        "plt.title(\"CatBoost - Actual vs Predicted Traffic Volume\")\n",
        "plt.xlabel(\"Sample\")\n",
        "plt.ylabel(\"Volume\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ia8fZnmImUYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 10: Create heatmap classification DataFrame\n",
        "heatmap_df = pd.DataFrame({\n",
        "    'Predicted': y_pred,\n",
        "    'hour': X_test['hour'].values,\n",
        "    'weekday': X_test['weekday'].values\n",
        "})"
      ],
      "metadata": {
        "id": "UPikV2FVmYCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 11: Classify traffic levels\n",
        "def classify_traffic(volume):\n",
        "    if volume < 300:\n",
        "        return 'Low'\n",
        "    elif volume < 700:\n",
        "        return 'Medium'\n",
        "    else:\n",
        "        return 'High'\n",
        "\n",
        "heatmap_df['Traffic_Level'] = heatmap_df['Predicted'].apply(classify_traffic)"
      ],
      "metadata": {
        "id": "51DwVRZ9mX33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Create pivot table for heatmap\n",
        "pivot_class = heatmap_df.pivot_table(\n",
        "    values='Traffic_Level',\n",
        "    index='hour',\n",
        "    columns='weekday',\n",
        "    aggfunc=lambda x: x.mode()[0] if not x.mode().empty else 'Low'\n",
        ")"
      ],
      "metadata": {
        "id": "2cTs8ByKmXtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 13: Mapping classes to numeric values for color encoding\n",
        "class_to_num = {'Low': 0, 'Medium': 1, 'High': 2}\n",
        "pivot_numeric = pivot_class.replace(class_to_num)"
      ],
      "metadata": {
        "id": "VZP_fxNKmXhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 14: Define color map for numeric encoding\n",
        "cmap = ListedColormap(['#56B4E9', '#F0E442', '#D55E00'])  # Blue, Yellow, Red"
      ],
      "metadata": {
        "id": "lryVTBlfmXVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 15: Plot heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(pivot_numeric, cmap=cmap, annot=pivot_class, fmt='', cbar=False)\n",
        "plt.title(\" CatBoost - Traffic Intensity by Hour & Weekday\")\n",
        "plt.xlabel(\"Weekday (0 = Monday)\")\n",
        "plt.ylabel(\"Hour\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K__QlyAnmXJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}